import re
import requests
from collections import defaultdict
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import spacy
from Levenshtein import distance as lev_distance


# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –º–∞—Ç–µ—Ä–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ GitHub + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
def load_ban_words():
    url = "https://raw.githubusercontent.com/bars38/Russian_ban_words/master/words.txt"
    try:
        response = requests.get(url)
        response.raise_for_status()
        words = [word.strip().lower() for word in response.text.split('\n') if word.strip()]

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        extra_words = [
            "–Ω–∞—Ö—É–π", "–Ω–∞–∑—É–π", "–ø–µ–∑–¥–∞", "–±–ª—è—Ç–±", "–Ω–Ω–∞–∑—É–π", "–Ω–Ω–∞—Ö—É–π",
            "–ø–∏–¥–æ—Ä", "–±—É–¥", "–∫–∞–≥", "–∫–∞–≥–∞", "–ø–µ–¥–∏–∫", "–µ–±–∞–Ω—ã–π", "–µ–±–∞–Ω–Ω—ã–π", "–µ–±", "—Å—É–∫–∞", "–±–ª—è—Ç"
        ]
        words.extend(extra_words)

        return set(words), words
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤: {e}")
        base_words = [
            "—Ö—É–π", "–ø–∏–∑–¥–∞", "–µ–±–∞–ª", "—Å—É–∫–∞", "–±–ª—è–¥—å", "–º—É–¥–∞–∫", "–∑–∞–ª—É–ø–∞",
            "–Ω–∞—Ö—É–π", "–Ω–∞–∑—É–π", "–ø–µ–∑–¥–∞", "–±–ª—è—Ç–±", "–Ω–Ω–∞–∑—É–π", "–Ω–Ω–∞—Ö—É–π",
            "–ø–∏–¥–æ—Ä", "–±—É–¥", "–∫–∞–≥", "–∫–∞–≥–∞", "–ø–µ–¥–∏–∫", "–µ–±–∞–Ω—ã–π", "–µ–±–∞–Ω–Ω—ã–π", "–µ–±", "–±–ª—è—Ç"
        ]
        return set(base_words), base_words


BAN_WORDS_SET, BAN_WORDS_LIST = load_ban_words()
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(BAN_WORDS_SET)} –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy
try:
    nlp = spacy.load("ru_core_news_sm", disable=["parser", "ner"])
    SPACY_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ spaCy: {e}")
    SPACY_AVAILABLE = False


# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤
def generate_smart_patterns(words):
    patterns = []
    short_words = set()

    for word in words:
        if len(word) <= 3:  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            short_words.add(word)
            continue

        # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        patterns.append(r"(?i)\b" + re.escape(word) + r"\b")

        # –ü–∞—Ç—Ç–µ—Ä–Ω —Å –∑–∞–º–µ–Ω–æ–π –ø–æ—Ö–æ–∂–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        pattern = []
        for char in word:
            if char in {'–∞', 'a', '@'}:
                pattern.append('[–∞a@]')
            elif char in {'–æ', 'o', '0'}:
                pattern.append('[–æo0]')
            elif char in {'–µ', 'e', '—ë'}:
                pattern.append('[–µe—ë]')
            elif char in {'–∏', 'i', '—ã'}:
                pattern.append('[–∏i—ã]')
            else:
                pattern.append(re.escape(char))
        patterns.append(r"(?i)\b" + ''.join(pattern) + r"\b")

    return patterns, short_words


PATTERNS, SHORT_WORDS = generate_smart_patterns(BAN_WORDS_LIST)


# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
def contains_bad_content(text: str) -> bool:
    text_lower = text.lower()

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    if any(word in text_lower for word in SHORT_WORDS):
        return True

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    for pattern in PATTERNS:
        if re.search(pattern, text_lower):
            return True

    # NLP –∞–Ω–∞–ª–∏–∑
    if SPACY_AVAILABLE and len(text_lower) > 3:
        try:
            doc = nlp(text_lower)
            for token in doc:
                if token.lemma_ in BAN_WORDS_SET or token.text in BAN_WORDS_SET:
                    return True
                if len(token.text) > 3:
                    closest_word = min(BAN_WORDS_SET, key=lambda x: lev_distance(token.text, x))
                    if lev_distance(token.text, closest_word) <= 1:
                        return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ NLP: {e}")

    return False


# –°–∏—Å—Ç–µ–º–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
user_stats = defaultdict(int)
message_log = []


async def show_stats(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not user_stats:
        await update.message.reply_text("–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ")
        return

    stats_text = "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–π:\n"
    sorted_stats = sorted(user_stats.items(), key=lambda x: x[1], reverse=True)

    for user_id, count in sorted_stats[:20]:
        try:
            user = await context.bot.get_chat_member(update.message.chat_id, user_id)
            name = user.user.first_name or user.user.username or f"ID:{user_id}"
            stats_text += f"üî¥ {name}: {count} –Ω–∞—Ä—É—à–µ–Ω–∏–π\n"
        except:
            stats_text += f"üî¥ ID:{user_id}: {count} –Ω–∞—Ä—É—à–µ–Ω–∏–π\n"

    await update.message.reply_text(stats_text)


async def log_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message or not update.message.text:
        return

    user = update.message.from_user
    text = update.message.text

    if contains_bad_content(text):
        user_stats[user.id] += 1
        log_entry = f"–ù–∞—Ä—É—à–µ–Ω–∏–µ –æ—Ç {user.id} (@{user.username or '–Ω–µ—Ç'}): '{text}'"
        message_log.append(log_entry)
        print(log_entry)


def main():
    application = Application.builder().token("7777974311:AAEeEkhcjo34GJkIJptPsFeSW1-ZWz9sNKk").build()

    application.add_handler(CommandHandler("statkarma", show_stats))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, log_message))

    print("üü¢ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û–∂–∏–¥–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π...")
    application.run_polling()


if __name__ == "__main__":
    main()